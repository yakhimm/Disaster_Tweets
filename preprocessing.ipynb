{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "with open('./dataset/stopwords.txt') as f:\n",
    "    stopwords_list = []\n",
    "    for row in f:\n",
    "        stopwords_list.append(row.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "def remove_URL(text):\n",
    "    # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "# remove emoji\n",
    "def remove_emojis(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    text = re.sub(emoj, '', text)\n",
    "    text = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|p|v|o|x|3|d|)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # stopwords_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "# Kí hiệu\n",
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]',' ',text)\n",
    "\n",
    "# Xoá khoảng trắng\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Tag \n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@\\S+','',text)\n",
    "    # return re.sub(r'@','',text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for i in range(0,len(word_tokens)):\n",
    "        word_tokens[i] = lemmatizer.lemmatize(word_tokens[i], 'v')\n",
    "    return ' '.join([word for word in word_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\",\n",
    "                \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "                \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "                \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "                \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\",\n",
    "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center',\n",
    "                'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling',\n",
    "                'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', \n",
    "                'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', \n",
    "                'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', \n",
    "                'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n",
    "                'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate',\n",
    "                \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "def replace_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "# abbreviations = {,\n",
    "#     \"4ao\" : \"for adults only\",\n",
    "#     \"a3\" : \"anytime anywhere anyplace\",\n",
    "#     \"aamof\" : \"as a matter of fact\",\n",
    "#     \"acct\" : \"account\",\n",
    "#     \"adih\" : \"another day in hell\",\n",
    "#     \"afaic\" : \"as far as i am concerned\",\n",
    "#     \"afaict\" : \"as far as i can tell\",\n",
    "#     \"afaik\" : \"as far as i know\",\n",
    "#     \"afair\" : \"as far as i remember\",\n",
    "#     \"afk\" : \"away from keyboard\",\n",
    "#     \"app\" : \"application\",\n",
    "#     \"approx\" : \"approximately\",\n",
    "#     \"apps\" : \"applications\",\n",
    "#     \"asap\" : \"as soon as possible\",\n",
    "#     \"asl\" : \"age sex location\",\n",
    "#     \"atk\" : \"at the keyboard\",\n",
    "#     \"ave.\" : \"avenue\",\n",
    "#     \"aymm\" : \"are you my mother\",\n",
    "#     \"ayor\" : \"at your own risk\", \n",
    "#     \"b&b\" : \"bed and breakfast\",\n",
    "#     \"b+b\" : \"bed and breakfast\",\n",
    "#     \"b.c\" : \"before christ\",\n",
    "#     \"b2b\" : \"business to business\",\n",
    "#     \"b2c\" : \"business to customer\",\n",
    "#     \"b4\" : \"before\",\n",
    "#     \"b4n\" : \"bye for now\",\n",
    "#     \"b@u\" : \"back at you\",\n",
    "#     \"bae\" : \"before anyone else\",\n",
    "#     \"bak\" : \"back at keyboard\",\n",
    "#     \"bbbg\" : \"bye bye be good\",\n",
    "#     \"bbc\" : \"british broadcasting corporation\",\n",
    "#     \"bbias\" : \"be back in a second\",\n",
    "#     \"bbl\" : \"be back later\",\n",
    "#     \"bbs\" : \"be back soon\",\n",
    "#     \"be4\" : \"before\",\n",
    "#     \"bfn\" : \"bye for now\",\n",
    "#     \"blvd\" : \"boulevard\",\n",
    "#     \"bout\" : \"about\",\n",
    "#     \"brb\" : \"be right back\",\n",
    "#     \"bros\" : \"brothers\",\n",
    "#     \"brt\" : \"be right there\",\n",
    "#     \"bsaaw\" : \"big smile and a wink\",\n",
    "#     \"btw\" : \"by the way\",\n",
    "#     \"bwl\" : \"bursting with laughter\",\n",
    "#     \"c/o\" : \"care of\",\n",
    "#     \"cet\" : \"central european time\",\n",
    "#     \"cf\" : \"compare\",\n",
    "#     \"cia\" : \"central intelligence agency\",\n",
    "#     \"csl\" : \"can not stop laughing\",\n",
    "#     \"cu\" : \"see you\",\n",
    "#     \"cul8r\" : \"see you later\",\n",
    "#     \"cv\" : \"curriculum vitae\",\n",
    "#     \"cwot\" : \"complete waste of time\",\n",
    "#     \"cya\" : \"see you\",\n",
    "#     \"cyt\" : \"see you tomorrow\",\n",
    "#     \"dae\" : \"does anyone else\",\n",
    "#     \"dbmib\" : \"do not bother me i am busy\",\n",
    "#     \"diy\" : \"do it yourself\",\n",
    "#     \"dm\" : \"direct message\",\n",
    "#     \"dwh\" : \"during work hours\",\n",
    "#     \"e123\" : \"easy as one two three\",\n",
    "#     \"eet\" : \"eastern european time\",\n",
    "#     \"eg\" : \"example\",\n",
    "#     \"embm\" : \"early morning business meeting\",\n",
    "#     \"encl\" : \"enclosed\",\n",
    "#     \"encl.\" : \"enclosed\",\n",
    "#     \"etc\" : \"and so on\",\n",
    "#     \"faq\" : \"frequently asked questions\",\n",
    "#     \"fawc\" : \"for anyone who cares\",\n",
    "#     \"fb\" : \"facebook\",\n",
    "#     \"fc\" : \"fingers crossed\",\n",
    "#     \"fig\" : \"figure\",\n",
    "#     \"fimh\" : \"forever in my heart\", \n",
    "#     \"ft.\" : \"feet\",\n",
    "#     \"ft\" : \"featuring\",\n",
    "#     \"ftl\" : \"for the loss\",\n",
    "#     \"ftw\" : \"for the win\",\n",
    "#     \"fwiw\" : \"for what it is worth\",\n",
    "#     \"fyi\" : \"for your information\",\n",
    "#     \"g9\" : \"genius\",\n",
    "#     \"gahoy\" : \"get a hold of yourself\",\n",
    "#     \"gal\" : \"get a life\",\n",
    "#     \"gcse\" : \"general certificate of secondary education\",\n",
    "#     \"gfn\" : \"gone for now\",\n",
    "#     \"gg\" : \"good game\",\n",
    "#     \"gl\" : \"good luck\",\n",
    "#     \"glhf\" : \"good luck have fun\",\n",
    "#     \"gmt\" : \"greenwich mean time\",\n",
    "#     \"gmta\" : \"great minds think alike\",\n",
    "#     \"gn\" : \"good night\",\n",
    "#     \"g.o.a.t\" : \"greatest of all time\",\n",
    "#     \"goat\" : \"greatest of all time\",\n",
    "#     \"goi\" : \"get over it\",\n",
    "#     \"gps\" : \"global positioning system\",\n",
    "#     \"gr8\" : \"great\",\n",
    "#     \"gratz\" : \"congratulations\",\n",
    "#     \"gyal\" : \"girl\",\n",
    "#     \"hc\" : \"hot and cold\",\n",
    "#     \"hp\" : \"horsepower\",\n",
    "#     \"hr\" : \"hour\",\n",
    "#     \"hrh\" : \"his royal highness\",\n",
    "#     \"ht\" : \"height\",\n",
    "#     \"ibrb\" : \"i will be right back\",\n",
    "#     \"ic\" : \"i see\",\n",
    "#     \"icq\" : \"i seek you\",\n",
    "#     \"icymi\" : \"in case you missed it\",\n",
    "#     \"idc\" : \"i do not care\",\n",
    "#     \"idgadf\" : \"i do not give a damn fuck\",\n",
    "#     \"idgaf\" : \"i do not give a fuck\",\n",
    "#     \"idk\" : \"i do not know\",\n",
    "#     \"ie\" : \"that is\",\n",
    "#     \"i.e\" : \"that is\",\n",
    "#     \"ifyp\" : \"i feel your pain\",\n",
    "#     \"IG\" : \"instagram\",\n",
    "#     \"iirc\" : \"if i remember correctly\",\n",
    "#     \"ilu\" : \"i love you\",\n",
    "#     \"ily\" : \"i love you\",\n",
    "#     \"imho\" : \"in my humble opinion\",\n",
    "#     \"imo\" : \"in my opinion\",\n",
    "#     \"imu\" : \"i miss you\",\n",
    "#     \"iow\" : \"in other words\",\n",
    "#     \"irl\" : \"in real life\",\n",
    "#     \"j4f\" : \"just for fun\",\n",
    "#     \"jic\" : \"just in case\",\n",
    "#     \"jk\" : \"just kidding\",\n",
    "#     \"jsyk\" : \"just so you know\",\n",
    "#     \"l8r\" : \"later\",\n",
    "#     \"lb\" : \"pound\",\n",
    "#     \"lbs\" : \"pounds\",\n",
    "#     \"ldr\" : \"long distance relationship\",\n",
    "#     \"lmao\" : \"laugh my ass off\",\n",
    "#     \"lmfao\" : \"laugh my fucking ass off\",\n",
    "#     \"lol\" : \"laughing out loud\",\n",
    "#     \"ltd\" : \"limited\",\n",
    "#     \"ltns\" : \"long time no see\",\n",
    "#     \"m8\" : \"mate\",\n",
    "#     \"mf\" : \"motherfucker\",\n",
    "#     \"mfs\" : \"motherfuckers\",\n",
    "#     \"mfw\" : \"my face when\",\n",
    "#     \"mofo\" : \"motherfucker\",\n",
    "#     \"mph\" : \"miles per hour\",\n",
    "#     \"mr\" : \"mister\",\n",
    "#     \"mrw\" : \"my reaction when\",\n",
    "#     \"ms\" : \"miss\",\n",
    "#     \"mte\" : \"my thoughts exactly\",\n",
    "#     \"nagi\" : \"not a good idea\",\n",
    "#     \"nbc\" : \"national broadcasting company\",\n",
    "#     \"nbd\" : \"not big deal\",\n",
    "#     \"nfs\" : \"not for sale\",\n",
    "#     \"ngl\" : \"not going to lie\",\n",
    "#     \"nhs\" : \"national health service\",\n",
    "#     \"nrn\" : \"no reply necessary\",\n",
    "#     \"nsfl\" : \"not safe for life\",\n",
    "#     \"nsfw\" : \"not safe for work\",\n",
    "#     \"nth\" : \"nice to have\",\n",
    "#     \"nvr\" : \"never\",\n",
    "#     \"nyc\" : \"new york city\",\n",
    "#     \"oc\" : \"original content\",\n",
    "#     \"og\" : \"original\",\n",
    "#     \"ohp\" : \"overhead projector\",\n",
    "#     \"oic\" : \"oh i see\",\n",
    "#     \"omdb\" : \"over my dead body\",\n",
    "#     \"omg\" : \"oh my god\",\n",
    "#     \"omw\" : \"on my way\",\n",
    "#     \"p.a\" : \"per annum\",\n",
    "#     \"p.m\" : \"after midday\",\n",
    "#     \"pm\" : \"prime minister\",\n",
    "#     \"poc\" : \"people of color\",\n",
    "#     \"pov\" : \"point of view\",\n",
    "#     \"pp\" : \"pages\",\n",
    "#     \"ppl\" : \"people\",\n",
    "#     \"prw\" : \"parents are watching\",\n",
    "#     \"ps\" : \"postscript\",\n",
    "#     \"pt\" : \"point\",\n",
    "#     \"ptb\" : \"please text back\",\n",
    "#     \"pto\" : \"please turn over\",\n",
    "#     \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "#     \"ratchet\" : \"rude\",\n",
    "#     \"rbtl\" : \"read between the lines\",\n",
    "#     \"rlrt\" : \"real life retweet\", \n",
    "#     \"rofl\" : \"rolling on the floor laughing\",\n",
    "#     \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "#     \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "#     \"rt\" : \"retweet\",\n",
    "#     \"ruok\" : \"are you ok\",\n",
    "#     \"sfw\" : \"safe for work\",\n",
    "#     \"sk8\" : \"skate\",\n",
    "#     \"smh\" : \"shake my head\",\n",
    "#     \"sq\" : \"square\",\n",
    "#     \"srsly\" : \"seriously\", \n",
    "#     \"ssdd\" : \"same stuff different day\",\n",
    "#     \"tbh\" : \"to be honest\",\n",
    "#     \"tbs\" : \"tablespooful\",\n",
    "#     \"tbsp\" : \"tablespooful\",\n",
    "#     \"tfw\" : \"that feeling when\",\n",
    "#     \"thks\" : \"thank you\",\n",
    "#     \"tho\" : \"though\",\n",
    "#     \"thx\" : \"thank you\",\n",
    "#     \"tia\" : \"thanks in advance\",\n",
    "#     \"til\" : \"today i learned\",\n",
    "#     \"tl;dr\" : \"too long i did not read\",\n",
    "#     \"tldr\" : \"too long i did not read\",\n",
    "#     \"tmb\" : \"tweet me back\",\n",
    "#     \"tntl\" : \"trying not to laugh\",\n",
    "#     \"ttyl\" : \"talk to you later\",\n",
    "#     \"u\" : \"you\",\n",
    "#     \"u2\" : \"you too\",\n",
    "#     \"u4e\" : \"yours for ever\",\n",
    "#     \"utc\" : \"coordinated universal time\",\n",
    "#     \"w/\" : \"with\",\n",
    "#     \"w/o\" : \"without\",\n",
    "#     \"w8\" : \"wait\",\n",
    "#     \"wassup\" : \"what is up\",\n",
    "#     \"wb\" : \"welcome back\",\n",
    "#     \"wtf\" : \"what the fuck\",\n",
    "#     \"wtg\" : \"way to go\",\n",
    "#     \"wtpa\" : \"where the party at\",\n",
    "#     \"wuf\" : \"where are you from\",\n",
    "#     \"wuzup\" : \"what is up\",\n",
    "#     \"wywh\" : \"wish you were here\",\n",
    "#     \"yd\" : \"yard\",\n",
    "#     \"ygtr\" : \"you got that right\",\n",
    "#     \"ynk\" : \"you never know\",\n",
    "#     \"zzz\" : \"sleeping bored and tired\",\n",
    "# }\n",
    "\n",
    "# def convert_abbrev(word):\n",
    "#     return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "# def convert_abbrev_in_text(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [convert_abbrev(word) for word in tokens]\n",
    "#     text = ' '.join(tokens)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df):\n",
    "    returned_df = df.copy()\n",
    "    dict_func = [\n",
    "                to_lower,\n",
    "                replace_misspell,   \n",
    "                remove_URL,\n",
    "                remove_mention,\n",
    "                remove_emojis,\n",
    "                # invalid_char + whitspace = punct \n",
    "                remove_invalid_char,\n",
    "                remove_leading_whitespace,\n",
    "                # convert_abbrev_in_text, \n",
    "                remove_stopwords,\n",
    "                lemmatize,\n",
    "    ]\n",
    "    for func in dict_func:\n",
    "        returned_df['text'] = returned_df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    returned_df['keyword'] = returned_df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_csv(tweet)\n",
    "train_df = group_similar_texts_with_same_loc(df, 0.9)\n",
    "empty_text_rows = train_df[train_df['text'] == '']\n",
    "train_df = train_df.drop(empty_text_rows.index)\n",
    "train_df['text'] = train_df['text'].replace('','none')\n",
    "train_df.to_csv('./preprocessing/train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = preprocessing_csv(test)\n",
    "df_test['text'] = df_test['text'].replace('','none')\n",
    "df_test.to_csv('./preprocessing/test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
