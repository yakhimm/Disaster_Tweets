{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "def remove_URL(text):\n",
    "    # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    with open('stopwords.txt') as f:\n",
    "        stopwords_list = []\n",
    "        for row in f:\n",
    "            stopwords_list.append(row.rstrip('\\n'))\n",
    "        # stopwords_list = stopwords.words('english')\n",
    "        return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@\\S+','',text)\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return lemmatizer.lemmatize(text)\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    return ' '.join([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours\\tourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'top hill can see fire woods...'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"I'm on top of the hill and I can see a fire in the woods...\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df, type_file = 'train'):\n",
    "    dict_func = {\n",
    "                to_lower,\n",
    "                remove_stopwords,\n",
    "                remove_URL,\n",
    "                remove_html,\n",
    "                # remove_emoji,\n",
    "                # remove_punct,\n",
    "                remove_mention,\n",
    "                remove_leading_whitespace,\n",
    "                remove_invalid_char,\n",
    "                #  lemmatize,\n",
    "                # stemming\n",
    "    } \n",
    "    for func in dict_func:\n",
    "        df['text'] = df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    \n",
    "    df['keyword'] = df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    # df.to_csv(f'./preprocessing/{type_file}.csv', index = False)\n",
    "\n",
    "    # print(f'Tiền xử lý vào ghi dữ liệu của tập {type_file} thành công !!')\n",
    "    if(type_file == 'train'):\n",
    "        empty_text_rows = df[df['text'] == '']\n",
    "        df = df.drop(empty_text_rows.index)\n",
    "    # elif(type == 'test'):\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                      13\n",
       "keyword                                NaN\n",
       "location                               NaN\n",
       "text        im top hill can see fire woods\n",
       "target                                   1\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing_csv(tweet, 'train')\n",
    "# df.to_csv(f'./preprocessing/train.csv', index = False)\n",
    "df.loc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_duplicate_text(df=df, outFile=\"identical_rows.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text_with_same_loc(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4497",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 4497",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\HK2-2223\\PTDLTM\\Disaster_Tweets\\preprocessing.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m out_duplicate_text_with_same_loc(df\u001b[39m=\u001b[39;49mpreprocessing_csv(tweet),outFile\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39midentical_rows_with_same_loc.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\HK2-2223\\PTDLTM\\Disaster_Tweets\\preprocessing.ipynb Cell 11\u001b[0m in \u001b[0;36mout_duplicate_text_with_same_loc\u001b[1;34m(df, outFile)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m group_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mif\u001b[39;00m cosine_sim[i,j] \u001b[39m>\u001b[39m threshold \u001b[39mand\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m==\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m'\u001b[39;49m][j]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mif\u001b[39;00m group_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HK2-2223/PTDLTM/Disaster_Tweets/preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m             group_id \u001b[39m=\u001b[39m j\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 4497"
     ]
    }
   ],
   "source": [
    "out_duplicate_text_with_same_loc(df=preprocessing_csv(tweet),outFile='identical_rows_with_same_loc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = preprocessing_csv(tweet)\n",
    "\n",
    "# # Define a function to group similar text\n",
    "# def group_similar_text(df, threshold=0.7):\n",
    "#     # Use TfidfVectorizer to transform the text into a vector representation\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     vectors = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "#     # Use cosine_similarity to calculate pairwise similarities between the vectors\n",
    "#     similarities = cosine_similarity(vectors)\n",
    "\n",
    "#     # Create a dictionary to store the groups\n",
    "#     groups = {}\n",
    "#     for i in range(len(df)):\n",
    "#         found_group = False\n",
    "#         id = df.loc[i, 'id']\n",
    "#         keyword = df.loc[i, 'keyword']\n",
    "#         location = df.loc[i, 'location']\n",
    "#         text = df.loc[i, 'text']\n",
    "#         target = df.loc[i, 'target']\n",
    "\n",
    "#         # Check if the text belongs to an existing group\n",
    "#         for group_text, group_target in groups.items():\n",
    "#             score = similarities[i, group_target[0]]\n",
    "#             if score >= threshold:\n",
    "#                 group_target.append(target)\n",
    "#                 found_group = True\n",
    "#                 break\n",
    "\n",
    "#         # If the text doesn't belong to an existing group, create a new one\n",
    "#         if not found_group:\n",
    "#             groups[text] = [i, id, keyword, location, target]\n",
    "        \n",
    "#     # Create a new dataframe with the most frequent target for each group\n",
    "#     new_data = {'id':[], 'keyword': [], 'location':[], 'text': [], 'target': [], 'group_size': []}\n",
    "#     for group_text, group_info in groups.items():\n",
    "#         # Get the indices of the rows in the group\n",
    "#         group_indices = [group_info[0]]\n",
    "#         for i in range(len(df)):\n",
    "#             if i != group_info[0] and similarities[i, group_info[0]] >= threshold:\n",
    "#                 group_indices.append(i)\n",
    "\n",
    "#         # Get the most frequent target in the group\n",
    "#         group_targets = [df.loc[i, 'target'] for i in group_indices]\n",
    "#         most_frequent_target = max(set(group_targets), key=group_targets.count)\n",
    "\n",
    "#         # Add the group to the new dataframe\n",
    "#         new_data['id'].append()\n",
    "#         new_data['text'].append(group_text)\n",
    "#         new_data['target'].append(most_frequent_target)\n",
    "#         new_data['group_size'].append(len(group_indices))\n",
    "\n",
    "#     # Create the final dataframe\n",
    "#     new_df = pd.DataFrame(new_data)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "# # Group the similar text in the dataframe\n",
    "# new_df = group_similar_text(df, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_df = group_similar_texts(df, 0.7)\n",
    "train1_df.to_csv(f'./preprocessing/train1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_loc_df = group_similar_texts_with_same_loc(df, 0.7)\n",
    "train_same_loc_df.to_csv(f'./preprocessing/train_same_loc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest control wild fires cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km s volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0          deeds reason earthquake may allah forgive us       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent photo ruby alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding bridge collapse nearb...       1  \n",
       "7609  ariaahrary thetawniest control wild fires cali...       1  \n",
       "7610                  m194 0104 utc5km s volcano hawaii       1  \n",
       "7611  police investigating ebike collided car little...       1  \n",
       "7612  latest homes razed northern california wildfir...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>10853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fatherofthree lost control car overtaking coll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6364</th>\n",
       "      <td>10854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 earthquake 9km ssw anza california iphone u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siren just went wasnt forney tornado warning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>officials say quarantine place alabama home po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flip side im walmart bomb everyone evacuate st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6368 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "6363  10853     NaN      NaN   \n",
       "6364  10854     NaN      NaN   \n",
       "6365  10860     NaN      NaN   \n",
       "6366  10862     NaN      NaN   \n",
       "6367  10864     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0          deeds reason earthquake may allah forgive us       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent photo ruby alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "6363  fatherofthree lost control car overtaking coll...       1  \n",
       "6364  13 earthquake 9km ssw anza california iphone u...       1  \n",
       "6365       siren just went wasnt forney tornado warning       1  \n",
       "6366  officials say quarantine place alabama home po...       1  \n",
       "6367  flip side im walmart bomb everyone evacuate st...       1  \n",
       "\n",
       "[6368 rows x 5 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest control wild fires cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km s volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7390 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7385  10869     NaN      NaN   \n",
       "7386  10870     NaN      NaN   \n",
       "7387  10871     NaN      NaN   \n",
       "7388  10872     NaN      NaN   \n",
       "7389  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0          deeds reason earthquake may allah forgive us       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent photo ruby alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7385  two giant cranes holding bridge collapse nearb...       1  \n",
       "7386  ariaahrary thetawniest control wild fires cali...       1  \n",
       "7387                  m194 0104 utc5km s volcano hawaii       1  \n",
       "7388  police investigating ebike collided car little...       1  \n",
       "7389  latest homes razed northern california wildfir...       1  \n",
       "\n",
       "[7390 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train_same_loc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
