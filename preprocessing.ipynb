{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "def remove_URL(text):\n",
    "    # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@\\S+','',text)\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return lemmatizer.lemmatize(text)\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    return ' '.join([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mani bear'"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(\"many bears\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df, type_file = 'train'):\n",
    "    dict_func = {remove_URL,\n",
    "                 to_lower,\n",
    "                 remove_html,\n",
    "                #  remove_emoji,\n",
    "                #  remove_punct,\n",
    "                 remove_stopwords,\n",
    "                 remove_mention,\n",
    "                 remove_leading_whitespace,\n",
    "                 remove_invalid_char,\n",
    "                #  lemmatize,\n",
    "                 stemming} \n",
    "    for func in dict_func:\n",
    "        df['text'] = df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    \n",
    "    df['keyword'] = df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    # df.to_csv(f'./preprocessing/{type_file}.csv', index = False)\n",
    "\n",
    "    # print(f'Tiền xử lý vào ghi dữ liệu của tập {type_file} thành công !!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_csv(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_duplicate_text(df=df, outFile=\"identical_rows.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text_with_same_loc(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_duplicate_text_with_same_loc(df=preprocessing_csv(tweet),outFile='identical_rows_with_same_loc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = preprocessing_csv(tweet)\n",
    "\n",
    "# # Define a function to group similar text\n",
    "# def group_similar_text(df, threshold=0.7):\n",
    "#     # Use TfidfVectorizer to transform the text into a vector representation\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     vectors = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "#     # Use cosine_similarity to calculate pairwise similarities between the vectors\n",
    "#     similarities = cosine_similarity(vectors)\n",
    "\n",
    "#     # Create a dictionary to store the groups\n",
    "#     groups = {}\n",
    "#     for i in range(len(df)):\n",
    "#         found_group = False\n",
    "#         id = df.loc[i, 'id']\n",
    "#         keyword = df.loc[i, 'keyword']\n",
    "#         location = df.loc[i, 'location']\n",
    "#         text = df.loc[i, 'text']\n",
    "#         target = df.loc[i, 'target']\n",
    "\n",
    "#         # Check if the text belongs to an existing group\n",
    "#         for group_text, group_target in groups.items():\n",
    "#             score = similarities[i, group_target[0]]\n",
    "#             if score >= threshold:\n",
    "#                 group_target.append(target)\n",
    "#                 found_group = True\n",
    "#                 break\n",
    "\n",
    "#         # If the text doesn't belong to an existing group, create a new one\n",
    "#         if not found_group:\n",
    "#             groups[text] = [i, id, keyword, location, target]\n",
    "        \n",
    "#     # Create a new dataframe with the most frequent target for each group\n",
    "#     new_data = {'id':[], 'keyword': [], 'location':[], 'text': [], 'target': [], 'group_size': []}\n",
    "#     for group_text, group_info in groups.items():\n",
    "#         # Get the indices of the rows in the group\n",
    "#         group_indices = [group_info[0]]\n",
    "#         for i in range(len(df)):\n",
    "#             if i != group_info[0] and similarities[i, group_info[0]] >= threshold:\n",
    "#                 group_indices.append(i)\n",
    "\n",
    "#         # Get the most frequent target in the group\n",
    "#         group_targets = [df.loc[i, 'target'] for i in group_indices]\n",
    "#         most_frequent_target = max(set(group_targets), key=group_targets.count)\n",
    "\n",
    "#         # Add the group to the new dataframe\n",
    "#         new_data['id'].append()\n",
    "#         new_data['text'].append(group_text)\n",
    "#         new_data['target'].append(most_frequent_target)\n",
    "#         new_data['group_size'].append(len(group_indices))\n",
    "\n",
    "#     # Create the final dataframe\n",
    "#     new_df = pd.DataFrame(new_data)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "# # Group the similar text in the dataframe\n",
    "# new_df = group_similar_text(df, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_df = group_similar_texts(df, 0.7)\n",
    "train1_df.to_csv(f'./preprocessing/train1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_loc_df = group_similar_texts_with_same_loc(df, 0.7)\n",
    "train_same_loc_df.to_csv(f'./preprocessing/train_same_loc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridg collap nearbi home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car littl portug eb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0         deed reason thi earthquak may allah forgiv us       1  \n",
       "1                  forest fire near la rong sask canada       1  \n",
       "2     resid ask shelter place notifi offic evacu she...       1  \n",
       "3     13000 peopl receiv wildfir evacu order california       1  \n",
       "4     got sent thi photo rubi alaska smoke wildfir p...       1  \n",
       "...                                                 ...     ...  \n",
       "7608      two giant crane hold bridg collap nearbi home       1  \n",
       "7609  control wild fire california even northern par...       1  \n",
       "7610                   m194 0104 utc 5km volcano hawaii       1  \n",
       "7611  polic investig ebik collid car littl portug eb...       1  \n",
       "7612  latest home raze northern california wildfir a...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridg collap nearbi home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car littl portug eb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0         deed reason thi earthquak may allah forgiv us       1  \n",
       "1                  forest fire near la rong sask canada       1  \n",
       "2     resid ask shelter place notifi offic evacu she...       1  \n",
       "3     13000 peopl receiv wildfir evacu order california       1  \n",
       "4     got sent thi photo rubi alaska smoke wildfir p...       1  \n",
       "...                                                 ...     ...  \n",
       "7608      two giant crane hold bridg collap nearbi home       1  \n",
       "7609  control wild fire california even northern par...       1  \n",
       "7610                   m194 0104 utc 5km volcano hawaii       1  \n",
       "7611  polic investig ebik collid car littl portug eb...       1  \n",
       "7612  latest home raze northern california wildfir a...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_csv(tweet).to_csv(f'./preprocessing/train.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6290</th>\n",
       "      <td>10851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rt nw issu sever thunderstorm warn part ar nc ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6291</th>\n",
       "      <td>10853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fatherofthr lost control car overtak collid ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>10854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 earthquak 9km ssw anza california iphon use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siren went wa nt forney tornado warn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flip side walmart bomb everyon evacu stay tune...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6295 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "6290  10851     NaN      NaN   \n",
       "6291  10853     NaN      NaN   \n",
       "6292  10854     NaN      NaN   \n",
       "6293  10860     NaN      NaN   \n",
       "6294  10864     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0         deed reason thi earthquak may allah forgiv us       1  \n",
       "1                  forest fire near la rong sask canada       1  \n",
       "2     resid ask shelter place notifi offic evacu she...       1  \n",
       "3     13000 peopl receiv wildfir evacu order california       1  \n",
       "4     got sent thi photo rubi alaska smoke wildfir p...       1  \n",
       "...                                                 ...     ...  \n",
       "6290  rt nw issu sever thunderstorm warn part ar nc ...       1  \n",
       "6291  fatherofthr lost control car overtak collid ba...       1  \n",
       "6292  13 earthquak 9km ssw anza california iphon use...       1  \n",
       "6293               siren went wa nt forney tornado warn       1  \n",
       "6294  flip side walmart bomb everyon evacu stay tune...       1  \n",
       "\n",
       "[6295 rows x 5 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7383</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridg collap nearbi home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car littl portug eb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7388 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7383  10869     NaN      NaN   \n",
       "7384  10870     NaN      NaN   \n",
       "7385  10871     NaN      NaN   \n",
       "7386  10872     NaN      NaN   \n",
       "7387  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0         deed reason thi earthquak may allah forgiv us       1  \n",
       "1                  forest fire near la rong sask canada       1  \n",
       "2     resid ask shelter place notifi offic evacu she...       1  \n",
       "3     13000 peopl receiv wildfir evacu order california       1  \n",
       "4     got sent thi photo rubi alaska smoke wildfir p...       1  \n",
       "...                                                 ...     ...  \n",
       "7383      two giant crane hold bridg collap nearbi home       1  \n",
       "7384  control wild fire california even northern par...       1  \n",
       "7385                   m194 0104 utc 5km volcano hawaii       1  \n",
       "7386  polic investig ebik collid car littl portug eb...       1  \n",
       "7387  latest home raze northern california wildfir a...       1  \n",
       "\n",
       "[7388 rows x 5 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train_same_loc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
