{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords_list = []\n",
    "    for row in f:\n",
    "        stopwords_list.append(row.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "def remove_URL(text):\n",
    "    # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "        # stopwords_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@\\S+','',text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    return ' '.join([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top hill can see fire woods...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"I'm on top of the hill and I can see a fire in the woods...\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df, type_file = 'train'):\n",
    "    returned_df = df.copy()\n",
    "    dict_func = [\n",
    "                to_lower,\n",
    "                remove_stopwords,\n",
    "                remove_URL,\n",
    "                remove_html,\n",
    "                # remove_emoji,\n",
    "                # remove_punct,\n",
    "                remove_mention,\n",
    "                remove_leading_whitespace,\n",
    "                remove_invalid_char,\n",
    "                #  lemmatize,\n",
    "                # stemming\n",
    "    ]\n",
    "    for func in dict_func:\n",
    "        returned_df['text'] = returned_df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    returned_df['keyword'] = returned_df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    # df.to_csv(f'./preprocessing/{type_file}.csv', index = False)\n",
    "\n",
    "    # print(f'Tiền xử lý vào ghi dữ liệu của tập {type_file} thành công !!')\n",
    "    if(type_file == 'train'):\n",
    "        empty_text_rows = returned_df[returned_df['text'] == '']\n",
    "        returned_df = returned_df.drop(empty_text_rows.index)\n",
    "    # elif(type == 'test'):\n",
    "        \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fires california even northern pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km s volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7611 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location   \n",
       "0         1     NaN      NaN  \\\n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0          deeds reason earthquake may allah forgive us       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent photo ruby alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding bridge collapse nearb...       1  \n",
       "7609  control wild fires california even northern pa...       1  \n",
       "7610                  m194 0104 utc5km s volcano hawaii       1  \n",
       "7611  police investigating ebike collided car little...       1  \n",
       "7612  latest homes razed northern california wildfir...       1  \n",
       "\n",
       "[7611 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing_csv(tweet, 'train')\n",
    "df.to_csv(f'./preprocessing/train.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4497",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4497",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_duplicate_text(df\u001b[39m=\u001b[39;49mdf, outFile\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39midentical_rows.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mout_duplicate_text\u001b[0;34m(df, outFile)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[39mif\u001b[39;00m group_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             group_id \u001b[39m=\u001b[39m j\n\u001b[0;32m---> 41\u001b[0m         \u001b[39melif\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m][j] \u001b[39m>\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m][group_id]:\n\u001b[1;32m     42\u001b[0m             group_id \u001b[39m=\u001b[39m j\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m group_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 4497"
     ]
    }
   ],
   "source": [
    "out_duplicate_text(df=df, outFile=\"identical_rows.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text_with_same_loc(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_duplicate_text_with_same_loc(df\u001b[39m=\u001b[39;49mpreprocessing_csv(tweet),outFile\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39midentical_rows_with_same_loc.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mout_duplicate_text_with_same_loc\u001b[0;34m(df, outFile)\u001b[0m\n\u001b[1;32m     36\u001b[0m group_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i):\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mif\u001b[39;00m cosine_sim[i,j] \u001b[39m>\u001b[39m threshold \u001b[39mand\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m==\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m][j]:\n\u001b[1;32m     39\u001b[0m         \u001b[39mif\u001b[39;00m group_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             group_id \u001b[39m=\u001b[39m j\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_duplicate_text_with_same_loc(df=preprocessing_csv(tweet),outFile='identical_rows_with_same_loc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = preprocessing_csv(tweet)\n",
    "\n",
    "# # Define a function to group similar text\n",
    "# def group_similar_text(df, threshold=0.7):\n",
    "#     # Use TfidfVectorizer to transform the text into a vector representation\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     vectors = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "#     # Use cosine_similarity to calculate pairwise similarities between the vectors\n",
    "#     similarities = cosine_similarity(vectors)\n",
    "\n",
    "#     # Create a dictionary to store the groups\n",
    "#     groups = {}\n",
    "#     for i in range(len(df)):\n",
    "#         found_group = False\n",
    "#         id = df.loc[i, 'id']\n",
    "#         keyword = df.loc[i, 'keyword']\n",
    "#         location = df.loc[i, 'location']\n",
    "#         text = df.loc[i, 'text']\n",
    "#         target = df.loc[i, 'target']\n",
    "\n",
    "#         # Check if the text belongs to an existing group\n",
    "#         for group_text, group_target in groups.items():\n",
    "#             score = similarities[i, group_target[0]]\n",
    "#             if score >= threshold:\n",
    "#                 group_target.append(target)\n",
    "#                 found_group = True\n",
    "#                 break\n",
    "\n",
    "#         # If the text doesn't belong to an existing group, create a new one\n",
    "#         if not found_group:\n",
    "#             groups[text] = [i, id, keyword, location, target]\n",
    "        \n",
    "#     # Create a new dataframe with the most frequent target for each group\n",
    "#     new_data = {'id':[], 'keyword': [], 'location':[], 'text': [], 'target': [], 'group_size': []}\n",
    "#     for group_text, group_info in groups.items():\n",
    "#         # Get the indices of the rows in the group\n",
    "#         group_indices = [group_info[0]]\n",
    "#         for i in range(len(df)):\n",
    "#             if i != group_info[0] and similarities[i, group_info[0]] >= threshold:\n",
    "#                 group_indices.append(i)\n",
    "\n",
    "#         # Get the most frequent target in the group\n",
    "#         group_targets = [df.loc[i, 'target'] for i in group_indices]\n",
    "#         most_frequent_target = max(set(group_targets), key=group_targets.count)\n",
    "\n",
    "#         # Add the group to the new dataframe\n",
    "#         new_data['id'].append()\n",
    "#         new_data['text'].append(group_text)\n",
    "#         new_data['target'].append(most_frequent_target)\n",
    "#         new_data['group_size'].append(len(group_indices))\n",
    "\n",
    "#     # Create the final dataframe\n",
    "#     new_df = pd.DataFrame(new_data)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "# # Group the similar text in the dataframe\n",
    "# new_df = group_similar_text(df, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train1_df \u001b[39m=\u001b[39m group_similar_texts(df, \u001b[39m0.7\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m train1_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./preprocessing/train1.csv\u001b[39m\u001b[39m'\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 54\u001b[0m, in \u001b[0;36mgroup_similar_texts\u001b[0;34m(df, threshold)\u001b[0m\n\u001b[1;32m     48\u001b[0m     most_common_target \u001b[39m=\u001b[39m target_freq\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m     representative \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[(df[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m most_common_target) \u001b[39m&\u001b[39m (df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39misin(group))]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]        \n\u001b[1;32m     50\u001b[0m     grouped_data\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     51\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: representative[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mkeyword\u001b[39m\u001b[39m\"\u001b[39m: representative[\u001b[39m\"\u001b[39m\u001b[39mkeyword\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: representative[\u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m---> 54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(df\u001b[39m.\u001b[39;49mloc[group][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: most_common_target\n\u001b[1;32m     56\u001b[0m     })\n\u001b[1;32m     57\u001b[0m grouped_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(grouped_data)\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m grouped_df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1330\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1334\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1271\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1273\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1464\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5871\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5868\u001b[0m     keyarr \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   5870\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 5871\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer_for(keyarr)\n\u001b[1;32m   5872\u001b[0m     keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(keyarr)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5858\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   5840\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5841\u001b[0m \u001b[39mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   5842\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5855\u001b[0m \u001b[39marray([0, 2])\u001b[39;00m\n\u001b[1;32m   5856\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 5858\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(target)\n\u001b[1;32m   5859\u001b[0m indexer, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   5860\u001b[0m \u001b[39mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3801\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3796\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3797\u001b[0m     \u001b[39mreturn\u001b[39;00m this\u001b[39m.\u001b[39m_get_indexer(\n\u001b[1;32m   3798\u001b[0m         target, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance\n\u001b[1;32m   3799\u001b[0m     )\n\u001b[0;32m-> 3801\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indexer(target, method, limit, tolerance)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/range.py:372\u001b[0m, in \u001b[0;36mRangeIndex._get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m    369\u001b[0m     reverse \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    370\u001b[0m     start, stop, step \u001b[39m=\u001b[39m reverse\u001b[39m.\u001b[39mstart, reverse\u001b[39m.\u001b[39mstop, reverse\u001b[39m.\u001b[39mstep\n\u001b[0;32m--> 372\u001b[0m target_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(target)\n\u001b[1;32m    373\u001b[0m locs \u001b[39m=\u001b[39m target_array \u001b[39m-\u001b[39m start\n\u001b[1;32m    374\u001b[0m valid \u001b[39m=\u001b[39m (locs \u001b[39m%\u001b[39m step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (locs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (target_array \u001b[39m<\u001b[39m stop)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train1_df = group_similar_texts(df, 0.7)\n",
    "train1_df.to_csv(f'./preprocessing/train1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_loc_df = group_similar_texts_with_same_loc(df, 0.7)\n",
    "train_same_loc_df.to_csv(f'./preprocessing/train_same_loc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./preprocessing/train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./preprocessing/train_same_loc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
