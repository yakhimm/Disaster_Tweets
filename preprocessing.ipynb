{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "def remove_URL(text):\n",
    "    # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    with open('stopwords.txt') as f:\n",
    "        stopwords_list = []\n",
    "        for row in f:\n",
    "            stopwords_list.append(row.rstrip('\\n'))\n",
    "        # stopwords_list = stopwords.words('english')\n",
    "        return ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@\\S+','',text)\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return lemmatizer.lemmatize(text)\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    return ' '.join([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loving'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"i'm loving u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df, type_file = 'train'):\n",
    "    dict_func = {remove_URL,\n",
    "                 to_lower,\n",
    "                 remove_html,\n",
    "                #  remove_emoji,\n",
    "                #  remove_punct,\n",
    "                 remove_stopwords,\n",
    "                 remove_mention,\n",
    "                 remove_leading_whitespace,\n",
    "                 remove_invalid_char,\n",
    "                #  lemmatize,\n",
    "                # stemming\n",
    "    } \n",
    "    for func in dict_func:\n",
    "        df['text'] = df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    \n",
    "    df['keyword'] = df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    # df.to_csv(f'./preprocessing/{type_file}.csv', index = False)\n",
    "\n",
    "    # print(f'Tiền xử lý vào ghi dữ liệu của tập {type_file} thành công !!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_csv(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_duplicate_text(df=df, outFile=\"identical_rows.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similar tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def out_duplicate_text_with_same_loc(df, outFile):\n",
    "# Create a sample dataframe\n",
    "# df = pd.DataFrame({'text': ['I love banana', 'I very love banana', 'I hate apples', 'I like oranges']})\n",
    "\n",
    "# Convert the text data into a matrix of TF-IDF features\n",
    "    # df = preprocessing_csv(tweet)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Calculate the cosine similarity between each pair of text data\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Define a threshold for similarity score\n",
    "    threshold = 0.7\n",
    "\n",
    "    # Group the text data based on similarity score\n",
    "    # groups = []\n",
    "    # visited = set()\n",
    "\n",
    "    # for i in range(len(cosine_sim)):\n",
    "    #     if i not in visited:\n",
    "    #         group = [i]\n",
    "    #         for j in range(i+1, len(cosine_sim)):\n",
    "    #             if cosine_sim[i][j] >= threshold and cosine_sim:\n",
    "    #                 group.append(j)\n",
    "    #                 visited.add(j)\n",
    "    #         groups.append(group)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(outFile, \"w\") as f:\n",
    "        for group in groups.values():\n",
    "            if(len(group) > 1):\n",
    "                f.write(f'Group {count}:')\n",
    "                for index in group:\n",
    "                    f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                    f.write('\\n')\n",
    "                    # print(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "                count = count + 1\n",
    "    # return groups\n",
    "    # # Print the groups\n",
    "    # with open(outFile, \"w\") as f:\n",
    "    #     for i, group in enumerate(groups):\n",
    "    #         if(len(group) > 1):\n",
    "    #             f.write(f'Group {i+1}:')\n",
    "    #             f.write('\\n')\n",
    "    #             for index in group:\n",
    "    #                 f.write(f' - id={df.iloc[index][\"id\"]}, {df.iloc[index][\"text\"]}, target={df.iloc[index][\"target\"]}, location={df.iloc[index][\"location\"]}, key={df.iloc[index][\"keyword\"]}')\n",
    "    #                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_duplicate_text_with_same_loc(df=preprocessing_csv(tweet),outFile='identical_rows_with_same_loc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = preprocessing_csv(tweet)\n",
    "\n",
    "# # Define a function to group similar text\n",
    "# def group_similar_text(df, threshold=0.7):\n",
    "#     # Use TfidfVectorizer to transform the text into a vector representation\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     vectors = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "#     # Use cosine_similarity to calculate pairwise similarities between the vectors\n",
    "#     similarities = cosine_similarity(vectors)\n",
    "\n",
    "#     # Create a dictionary to store the groups\n",
    "#     groups = {}\n",
    "#     for i in range(len(df)):\n",
    "#         found_group = False\n",
    "#         id = df.loc[i, 'id']\n",
    "#         keyword = df.loc[i, 'keyword']\n",
    "#         location = df.loc[i, 'location']\n",
    "#         text = df.loc[i, 'text']\n",
    "#         target = df.loc[i, 'target']\n",
    "\n",
    "#         # Check if the text belongs to an existing group\n",
    "#         for group_text, group_target in groups.items():\n",
    "#             score = similarities[i, group_target[0]]\n",
    "#             if score >= threshold:\n",
    "#                 group_target.append(target)\n",
    "#                 found_group = True\n",
    "#                 break\n",
    "\n",
    "#         # If the text doesn't belong to an existing group, create a new one\n",
    "#         if not found_group:\n",
    "#             groups[text] = [i, id, keyword, location, target]\n",
    "        \n",
    "#     # Create a new dataframe with the most frequent target for each group\n",
    "#     new_data = {'id':[], 'keyword': [], 'location':[], 'text': [], 'target': [], 'group_size': []}\n",
    "#     for group_text, group_info in groups.items():\n",
    "#         # Get the indices of the rows in the group\n",
    "#         group_indices = [group_info[0]]\n",
    "#         for i in range(len(df)):\n",
    "#             if i != group_info[0] and similarities[i, group_info[0]] >= threshold:\n",
    "#                 group_indices.append(i)\n",
    "\n",
    "#         # Get the most frequent target in the group\n",
    "#         group_targets = [df.loc[i, 'target'] for i in group_indices]\n",
    "#         most_frequent_target = max(set(group_targets), key=group_targets.count)\n",
    "\n",
    "#         # Add the group to the new dataframe\n",
    "#         new_data['id'].append()\n",
    "#         new_data['text'].append(group_text)\n",
    "#         new_data['target'].append(most_frequent_target)\n",
    "#         new_data['group_size'].append(len(group_indices))\n",
    "\n",
    "#     # Create the final dataframe\n",
    "#     new_df = pd.DataFrame(new_data)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "# # Group the similar text in the dataframe\n",
    "# new_df = group_similar_text(df, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    \"\"\"\n",
    "    Group similar texts in a DataFrame based on cosine similarity of TF-IDF vectors,\n",
    "    and keep the row with the highest frequency of target in each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        The input DataFrame containing the id, keyword, location, text, and target columns.\n",
    "    - threshold: float\n",
    "        The cosine similarity threshold above which texts are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - grouped_df: pandas DataFrame\n",
    "        A new DataFrame containing the text data for each group and the most common target,\n",
    "        as well as the id, keyword, and location columns of the row with the highest frequency of target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_df = group_similar_texts(df, 0.7)\n",
    "train1_df.to_csv(f'./preprocessing/train1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_loc_df = group_similar_texts_with_same_loc(df, 0.7)\n",
    "train_same_loc_df.to_csv(f'./preprocessing/train_same_loc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak allah forgiv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid shelter place notifi offic evacu shelte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>photo rubi alaska smoke wildfir pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant crane hold bridg collap nearbi http tco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrari thetawniest control wild californi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii http tcozdto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car portug ebik ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest raze northern california wildfir abc n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                    deed reason earthquak allah forgiv       1  \n",
       "1                            forest la rong sask canada       1  \n",
       "2      resid shelter place notifi offic evacu shelte...       1  \n",
       "3      13000 peopl receiv wildfir evacu order califo...       1  \n",
       "4           photo rubi alaska smoke wildfir pour school       1  \n",
       "...                                                 ...     ...  \n",
       "7608   giant crane hold bridg collap nearbi http tco...       1  \n",
       "7609   ariaahrari thetawniest control wild californi...       1  \n",
       "7610   m194 0104 utc 5km volcano hawaii http tcozdto...       1  \n",
       "7611   polic investig ebik collid car portug ebik ri...       1  \n",
       "7612   latest raze northern california wildfir abc n...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak allah forgiv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid shelter place notifi offic evacu shelte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>photo rubi alaska smoke wildfir pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant crane hold bridg collap nearbi http tco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrari thetawniest control wild californi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii http tcozdto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car portug ebik ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest raze northern california wildfir abc n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                    deed reason earthquak allah forgiv       1  \n",
       "1                            forest la rong sask canada       1  \n",
       "2      resid shelter place notifi offic evacu shelte...       1  \n",
       "3      13000 peopl receiv wildfir evacu order califo...       1  \n",
       "4           photo rubi alaska smoke wildfir pour school       1  \n",
       "...                                                 ...     ...  \n",
       "7608   giant crane hold bridg collap nearbi http tco...       1  \n",
       "7609   ariaahrari thetawniest control wild californi...       1  \n",
       "7610   m194 0104 utc 5km volcano hawaii http tcozdto...       1  \n",
       "7611   polic investig ebik collid car portug ebik ri...       1  \n",
       "7612   latest raze northern california wildfir abc n...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_csv(tweet).to_csv(f'./preprocessing/train.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak allah forgiv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid shelter place notifi offic evacu shelte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>photo rubi alaska smoke wildfir pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6684</th>\n",
       "      <td>10854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 earthquak 9km ssw anza california iphon us...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siren forney tornado warn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6686</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offici quarantin place alabama ebola case dev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6687</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flip walmart bomb evacu stay tune blow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suicid bomber kill 15 saudi secur site mosqu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6689 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "6684  10854     NaN      NaN   \n",
       "6685  10860     NaN      NaN   \n",
       "6686  10862     NaN      NaN   \n",
       "6687  10864     NaN      NaN   \n",
       "6688  10866     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                    deed reason earthquak allah forgiv       1  \n",
       "1                            forest la rong sask canada       1  \n",
       "2      resid shelter place notifi offic evacu shelte...       1  \n",
       "3      13000 peopl receiv wildfir evacu order califo...       1  \n",
       "4           photo rubi alaska smoke wildfir pour school       1  \n",
       "...                                                 ...     ...  \n",
       "6684   13 earthquak 9km ssw anza california iphon us...       1  \n",
       "6685                          siren forney tornado warn       1  \n",
       "6686   offici quarantin place alabama ebola case dev...       1  \n",
       "6687             flip walmart bomb evacu stay tune blow       1  \n",
       "6688   suicid bomber kill 15 saudi secur site mosqu ...       1  \n",
       "\n",
       "[6689 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak allah forgiv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid shelter place notifi offic evacu shelte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>photo rubi alaska smoke wildfir pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7423</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant crane hold bridg collap nearbi http tco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrari thetawniest control wild californi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc 5km volcano hawaii http tcozdto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car portug ebik ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest raze northern california wildfir abc n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7428 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7423  10869     NaN      NaN   \n",
       "7424  10870     NaN      NaN   \n",
       "7425  10871     NaN      NaN   \n",
       "7426  10872     NaN      NaN   \n",
       "7427  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                    deed reason earthquak allah forgiv       1  \n",
       "1                            forest la rong sask canada       1  \n",
       "2      resid shelter place notifi offic evacu shelte...       1  \n",
       "3      13000 peopl receiv wildfir evacu order califo...       1  \n",
       "4           photo rubi alaska smoke wildfir pour school       1  \n",
       "...                                                 ...     ...  \n",
       "7423   giant crane hold bridg collap nearbi http tco...       1  \n",
       "7424   ariaahrari thetawniest control wild californi...       1  \n",
       "7425   m194 0104 utc 5km volcano hawaii http tcozdto...       1  \n",
       "7426   polic investig ebik collid car portug ebik ri...       1  \n",
       "7427   latest raze northern california wildfir abc n...       1  \n",
       "\n",
       "[7428 rows x 5 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./preprocessing/train_same_loc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
