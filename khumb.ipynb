{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./preprocessing/train.csv')\n",
    "test_df = pd.read_csv('./preprocessing/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (6088, 1)\n",
      "X_val shape:  (1523, 1)\n",
      "y_train shape:  (6088, 1)\n",
      "y_val shape:  (1523, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets.drop(['id','keyword','location','target'],axis=1), tweets[['target']], test_size=0.2, stratify=tweets[['target']], random_state=0)\n",
    "X_train_text = X_train['text']\n",
    "X_val_text = X_val['text']\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_val shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = Tokenizer(num_words=5000, oov_token='<UNK>')\n",
    "tokenizer_1.fit_on_texts(X_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53, 14, 368, 6, 1295, 316, 1581], [265, 524, 127, 128, 220, 145, 302, 66, 111], [34, 818, 25, 81, 203, 4838], [4839, 1407, 819, 4, 4840, 394, 1582, 4841, 5, 4842, 1791, 35, 936, 116], [3277, 4843, 4844, 369, 671, 4845, 1088, 7, 204, 95, 76, 2096, 624, 112], [44, 1006, 2, 43, 107, 195, 132], [714, 273, 253, 764, 162, 873], [82, 4846, 2, 1179, 650, 237, 62, 4847, 163, 1792, 1180], [196, 2097, 3, 442], [146, 874, 303, 133, 1408, 266]]\n",
      "\n",
      "[[17, 2377, 1038, 1547, 1, 11, 67, 537, 1177, 1, 1323], [189, 5, 10, 121, 559, 222, 189, 3527, 1663, 2210, 3528, 2718, 3529], [1, 1, 798, 639, 514, 1914, 1], [729, 30, 792, 737, 738, 109, 547, 608, 12, 900, 970], [301, 125, 179, 9, 169, 42, 1058, 59, 359, 414, 372, 52, 801], [230, 417, 189, 11, 222, 703, 8, 436, 110, 480, 1284, 464, 121], [2778, 15, 55, 112, 1, 389, 587, 1, 42, 680, 56, 100, 1], [1495, 735, 343, 3163, 632, 179, 546, 343, 763, 20, 3163, 1, 1, 160], [60, 58, 82, 1143, 747, 686, 321, 952, 741, 817, 17, 82], [714, 273, 253, 764, 162, 873]]\n"
     ]
    }
   ],
   "source": [
    "X_train_text = tokenizer_1.texts_to_sequences(X_train_text)\n",
    "X_val_text = tokenizer_1.texts_to_sequences(X_val_text)\n",
    "print(X_train_text[:10])\n",
    "print('')\n",
    "print(X_val_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wreckage conclusively confirm mh370 malaysia pm investigators families be']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1.sequences_to_texts([X_train_text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Max Length: 25\n",
      "X_train shape: (6088, 50)\n",
      "X_train shape: (1523, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Train Set Max Length:', max(len(text) for text in X_train_text))\n",
    "maxlen = 50\n",
    "\n",
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train_text.shape)\n",
    "print('X_train shape:', X_val_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer_1.word_index) + 1\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./dataset/glove.twitter.27B.200d.txt', encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (11927, 200)\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training set\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "\n",
    "for word, i in tokenizer_1.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Embedding Matrix Shape:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "num_epochs=15\n",
    "dropout=0.2\n",
    "recurrent_dropout=0.2\n",
    "lr=0.0005\n",
    "batch_size=128\n",
    "class_weight = {0: y_train['target'].value_counts()[1]/len(y_train), 1: y_train['target'].value_counts()[0]/len(y_train)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 200)           2385400   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50, 128)           168448    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,685,561\n",
      "Trainable params: 300,161\n",
      "Non-trainable params: 2,385,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Environment\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)) # try adding dropout later\n",
    "lstm_model.add(LSTM(128))\n",
    "\n",
    "#model.add(Flatten())\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = optimizers.Adam(lr=lr)\n",
    "lstm_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(history):   \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(num_epochs), history.history['acc'],'-o',\n",
    "             label='Train ACC',color='#ff7f0e')\n",
    "    plt.plot(range(num_epochs),history.history['val_acc'],'-o',\n",
    "             label='Val ACC',color='#1f77b4')\n",
    "    x = np.argmax( history.history['val_acc'] ); y = np.max( history.history['val_acc'] )\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#1f77b4')\n",
    "    plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
    "    plt.ylabel('Accuracy',size=14); plt.xlabel('Epoch',size=14)\n",
    "    plt.legend(loc=(0.01,0.75))\n",
    "\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(range(num_epochs),history.history['loss'],'-o',\n",
    "              label='Train Loss',color='#2ca02c')\n",
    "    plt2.plot(range(num_epochs),history.history['val_loss'],'-o',\n",
    "              label='Val Loss',color='#d62728')\n",
    "    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#d62728')\n",
    "    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "   # plt.ylim([-0.2, 2])\n",
    "    plt.ylabel('Loss',size=14)\n",
    "    plt.xticks(ticks=list(range(num_epochs)),labels=list(range(1, num_epochs+1)))\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.01, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "48/48 [==============================] - 247s 5s/step - loss: 0.2796 - acc: 0.7198 - val_loss: 0.4887 - val_acc: 0.8037\n",
      "Epoch 2/15\n",
      "48/48 [==============================] - 229s 5s/step - loss: 0.2332 - acc: 0.7925 - val_loss: 0.4423 - val_acc: 0.8135\n",
      "Epoch 3/15\n",
      "48/48 [==============================] - 226s 5s/step - loss: 0.2229 - acc: 0.7947 - val_loss: 0.4564 - val_acc: 0.8102\n",
      "Epoch 4/15\n",
      "48/48 [==============================] - 221s 5s/step - loss: 0.2166 - acc: 0.8083 - val_loss: 0.4416 - val_acc: 0.8142\n",
      "Epoch 5/15\n",
      "48/48 [==============================] - 235s 5s/step - loss: 0.2126 - acc: 0.8098 - val_loss: 0.4515 - val_acc: 0.8056\n",
      "Epoch 6/15\n",
      "48/48 [==============================] - 238s 5s/step - loss: 0.2093 - acc: 0.8101 - val_loss: 0.4425 - val_acc: 0.8162\n",
      "Epoch 7/15\n",
      "48/48 [==============================] - 244s 5s/step - loss: 0.2066 - acc: 0.8113 - val_loss: 0.4273 - val_acc: 0.8168\n",
      "Epoch 8/15\n",
      "48/48 [==============================] - 220s 5s/step - loss: 0.2056 - acc: 0.8132 - val_loss: 0.4328 - val_acc: 0.8076\n",
      "Epoch 9/15\n",
      "48/48 [==============================] - 10822s 230s/step - loss: 0.1974 - acc: 0.8233 - val_loss: 0.4263 - val_acc: 0.8135\n",
      "Epoch 10/15\n",
      "48/48 [==============================] - 145s 3s/step - loss: 0.1987 - acc: 0.8192 - val_loss: 0.4438 - val_acc: 0.8135\n",
      "Epoch 11/15\n",
      "48/48 [==============================] - 223s 5s/step - loss: 0.1917 - acc: 0.8295 - val_loss: 0.4310 - val_acc: 0.8116\n",
      "Epoch 12/15\n",
      "48/48 [==============================] - 238s 5s/step - loss: 0.1828 - acc: 0.8412 - val_loss: 0.4593 - val_acc: 0.8162\n",
      "Epoch 13/15\n",
      "48/48 [==============================] - 238s 5s/step - loss: 0.1849 - acc: 0.8334 - val_loss: 0.4740 - val_acc: 0.8089\n",
      "Epoch 14/15\n",
      "48/48 [==============================] - 241s 5s/step - loss: 0.1774 - acc: 0.8472 - val_loss: 0.4421 - val_acc: 0.8089\n",
      "Epoch 15/15\n",
      "48/48 [==============================] - 225s 5s/step - loss: 0.1702 - acc: 0.8510 - val_loss: 0.4382 - val_acc: 0.8070\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_acc', save_best_only=True)\n",
    "history = lstm_model.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n",
    "                         class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (3263, 50)\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "test_text = test_df['text']\n",
    "test_text = tokenizer_1.texts_to_sequences(test_text)\n",
    "\n",
    "# padding\n",
    "test_text = pad_sequences(test_text, padding='post', maxlen=50)\n",
    "\n",
    "print('X_test shape:', test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 46s 432ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lstm prediction\n",
    "# model.predict(test_text)\n",
    "lstm_model.load_weights('lstm_model.h5')\n",
    "submission = test_df.copy()[['id']]\n",
    "submission['target'] = lstm_model.predict(test_text)\n",
    "def classes(x):\n",
    "    # sigmoid = 1/1+np.exp(-x)\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "submission['target'] = submission['target'].apply(lambda x: classes(x))\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965062825620595"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ans = pd.read_csv('./dataset/ans.csv')['target'].values\n",
    "accuracy_score(y_pred= submission['target'], y_true= ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 50, 200)           2385400   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 50, 256)          336896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  [(None, 50, 256),        394240    \n",
      " nal)                         (None, 128),                       \n",
      "                              (None, 128),                       \n",
      "                              (None, 128),                       \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      " attention_1 (attention)     (None, 256)               306       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,117,099\n",
      "Trainable params: 731,699\n",
      "Non-trainable params: 2,385,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Environment\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "### Attention\n",
    "\n",
    "## Hyperparameters\n",
    "num_epochs=15\n",
    "dropout=0.3\n",
    "recurrent_dropout=0.3\n",
    "lr=0.0005\n",
    "batch_size=128\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "\n",
    "## Embedding Layer\n",
    "sequence_input = Input(shape=(maxlen,))\n",
    "embedded_sequences = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "\n",
    "## RNN Layer\n",
    "lstm = Bidirectional(LSTM(128, return_sequences = True, dropout=dropout, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True))(lstm)\n",
    "\n",
    "## Attention Layer\n",
    "att_out=attention()(lstm)\n",
    "outputs=Dense(1,activation='sigmoid')(att_out)\n",
    "model_attn = Model(sequence_input, outputs)\n",
    "\n",
    "adam = optimizers.Adam(lr=lr)\n",
    "#sgd = optimizers.sgd(lr=lr)\n",
    "model_attn.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model_attn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "48/48 [==============================] - 959s 20s/step - loss: 0.2639 - acc: 0.7490 - val_loss: 0.4554 - val_acc: 0.8096\n",
      "Epoch 2/15\n",
      "48/48 [==============================] - 964s 20s/step - loss: 0.2260 - acc: 0.7925 - val_loss: 0.4437 - val_acc: 0.8109\n",
      "Epoch 3/15\n",
      "48/48 [==============================] - 742s 15s/step - loss: 0.2173 - acc: 0.7983 - val_loss: 0.4317 - val_acc: 0.8135\n",
      "Epoch 4/15\n",
      "48/48 [==============================] - 743s 16s/step - loss: 0.2113 - acc: 0.8075 - val_loss: 0.4437 - val_acc: 0.8089\n",
      "Epoch 5/15\n",
      "48/48 [==============================] - 874s 18s/step - loss: 0.2082 - acc: 0.8114 - val_loss: 0.4330 - val_acc: 0.8102\n",
      "Epoch 6/15\n",
      "48/48 [==============================] - 932s 19s/step - loss: 0.2039 - acc: 0.8152 - val_loss: 0.4383 - val_acc: 0.8188\n",
      "Epoch 7/15\n",
      "48/48 [==============================] - 890s 18s/step - loss: 0.1991 - acc: 0.8238 - val_loss: 0.4498 - val_acc: 0.8050\n",
      "Epoch 8/15\n",
      "42/48 [=========================>....] - ETA: 1:40 - loss: 0.1933 - acc: 0.8322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mattn_model.h5\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m history_attn \u001b[39m=\u001b[39m model_attn\u001b[39m.\u001b[39;49mfit(X_train_text, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, callbacks\u001b[39m=\u001b[39;49m[checkpoint], epochs\u001b[39m=\u001b[39;49mnum_epochs, \n\u001b[0;32m      3\u001b[0m                               class_weight\u001b[39m=\u001b[39;49mclass_weight, validation_data\u001b[39m=\u001b[39;49m(X_val_text, y_val), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# plot_model_performance(history_attn)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Environment\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('attn_model.h5', monitor='val_acc', save_best_only=True)\n",
    "history_attn = model_attn.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n",
    "                              class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\n",
    "# plot_model_performance(history_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (3263, 50)\n",
      "102/102 [==============================] - 70s 666ms/step\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "test_text = test_df['text']\n",
    "test_text = tokenizer_1.texts_to_sequences(test_text)\n",
    "\n",
    "# padding\n",
    "test_text = pad_sequences(test_text, padding='post', maxlen=50)\n",
    "\n",
    "print('X_test shape:', test_text.shape)\n",
    "\n",
    "model_attn.load_weights('attn_model.h5')\n",
    "\n",
    "submission = test_df.copy()[['id']]\n",
    "submission['target'] = model_attn.predict(test_text)\n",
    "\n",
    "def classes(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "submission['target'] = submission['target'].apply(lambda x: classes(x))\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7971192154459087"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = pd.read_csv('./dataset/ans.csv')['target'].values\n",
    "accuracy_score(y_pred= submission['target'], y_true= ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
